{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dzl3ves3qb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using local LLaMA model: llama3.1:8b\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "from chatgpt_parser import load_conversations, extract_linear_conversation\n",
    "\n",
    "FILE_PATH = \"data/conversations.json\"\n",
    "LLM_MODE = os.getenv(\"LLM_MODE\", \"openai\")\n",
    "\n",
    "if LLM_MODE == \"local\":\n",
    "    # Ollama client with OpenAI-compatible API\n",
    "    client = OpenAI(\n",
    "        base_url=\"http://localhost:11434/v1\",\n",
    "        api_key=\"ollama\"  # required but unused\n",
    "    )\n",
    "    MODEL_NAME = \"llama3.1:8b\"\n",
    "    print(f\"Using local LLaMA model: {MODEL_NAME}\")\n",
    "elif LLM_MODE == \"openai\":\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    MODEL_NAME = \"gpt-4o-mini\"\n",
    "    print(f\"Using OpenAI model: {MODEL_NAME}\")\n",
    "else:\n",
    "    raise ValueError(f\"Invalid LLM_MODE: {LLM_MODE}. Use 'local' or 'openai'\")\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9792791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(FILE_PATH, \"r\") as f:\n",
    "#     conversations = json.load(f)\n",
    "\n",
    "# print(f\"Loaded {len(conversations)} conversations\")\n",
    "# conversations[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82fd473f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3418\n"
     ]
    }
   ],
   "source": [
    "raw_data = load_conversations(FILE_PATH)\n",
    "print(len(raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f970dc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in extract_linear_conversation(raw_data[202]):\n",
    "#     print(f\"[{i['role']}] {i['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270f102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "convs_selected = random.sample(raw_data, 50)\n",
    "# convs_selected[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12b32a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_turn(turn: dict, agent_mode: str = 'summarised_conversation') -> str:\n",
    "    for s, t in turn.items():\n",
    "        if s == 'user':\n",
    "            return f\"[user]\\n{t}\\n\"\n",
    "        if s == agent_mode:\n",
    "            return f\"[agent]\\n{t}\\n\"\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rvfq6cgrlef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Create a simple prompt from the conversation\n",
    "base_prompt = \"\"\"\n",
    "You are an expert in summarizing content. You will be given a conversation between an agent and a human and your task is to summarise the last message content in a way that no content\n",
    "is lost but we avoid all the details that are not relevant to understand the conversation the user is having. give brief simple bullet points (no nested, no formatting) that cover the topic of what was talked.\n",
    "The content should be the minimum for someone reading the agent's summary to understand the users's response.\n",
    "Keep the language of the conversation.\n",
    "\n",
    "[OUTPUT EXAMPLE]\n",
    "- Explica al usuario qué evitar, cómo aliviar el dolor, qué alimentos consumir suaves.\n",
    "- Mejora típica en 48-72 horas, sugiere consultar con un médico si persisten síntomas graves.\n",
    "[END OF EXAMPLE]\n",
    "\n",
    "This is the turn to summarise:\n",
    "\"\"\"\n",
    "summarised_conversation = []\n",
    "from tqdm import tqdm\n",
    "for i, conv in enumerate(tqdm(convs_selected, desc=\"Processing conversations\")):\n",
    "    new_conversation_record = {\n",
    "        \"id\": conv['id'],\n",
    "        \"title\": conv['title'],\n",
    "        \"create_time\": conv['create_time'],\n",
    "        \"update_time\": conv['update_time'],\n",
    "        \"turns\": []\n",
    "    }\n",
    "    for message in extract_linear_conversation(conv):\n",
    "        if message['role'] == 'user':\n",
    "            # if len(message['text']) > 500:\n",
    "            #     summarise()\n",
    "            turn = {message['role']: message['text']}\n",
    "            print(turn)\n",
    "        elif message['role'] ==  'assistant':\n",
    "            prompt = base_prompt + \"\\n\".join([flatten_turn(turn) for turn in new_conversation_record['turns']]) + f\"{message['role']}: {message['text']}\"\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL_NAME,  # Use dynamic model name\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            turn = {message['role']: message['text'], \"summarised_conversation\": response.choices[0].message.content}\n",
    "            print(turn['summarised_conversation'])\n",
    "        new_conversation_record['turns'].append(turn)\n",
    "        print()\n",
    "    summarised_conversation.append(new_conversation_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8631c3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for conv in summarised_conversation:\n",
    "#     for turn in conv['turns']:\n",
    "#         print(flatten_turn(turn))\n",
    "#     print(\"--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e53067ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_FILE_NAME = \"summarised_conversation_debug.json\"\n",
    "\n",
    "def save_conversation_history(conversation_data: list, filename: str):\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            # Use indent=4 for a human-readable file format\n",
    "            json.dump(conversation_data, f, indent=4)\n",
    "        print(f\"Conversation history successfully saved to **{filename}**.\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error saving file **{filename}**: {e}\")\n",
    "\n",
    "def load_conversation_history(filename: str) -> list:\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"File **{filename}** not found. Starting with an empty conversation history.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            conversation_data = json.load(f)\n",
    "            print(f\"Conversation history successfully loaded from **{filename}**.\")\n",
    "            return conversation_data\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON from **{filename}**: {e}. Returning an empty list.\")\n",
    "        return []\n",
    "    except IOError as e:\n",
    "        print(f\"Error reading file **{filename}**: {e}. Returning an empty list.\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e626f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarised_conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "936e3354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation history successfully saved to **summarised_conversation_debug.json**.\n"
     ]
    }
   ],
   "source": [
    "save_conversation_history(summarised_conversation, DEBUG_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "608c675d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation history successfully loaded from **summarised_conversation_debug.json**.\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "samples_convs = load_conversation_history('summarised_conversation_debug.json')\n",
    "print(len(samples_convs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1245bc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [\n",
    "#     print(f\"[{r}] {t}\\n\")\n",
    "#     for conv in samples_convs\n",
    "#     for turns in conv['turns']\n",
    "#     for r, t in turns.items()\n",
    "# ];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b539b6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conversations: 100%|██████████| 50/50 [24:51<00:00, 29.84s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Create a simple prompt from the conversation\n",
    "base_prompt = \"\"\"\n",
    "[System Role] You are a dedicated Memory Manager. Your sole purpose is to extract actionable, long-term user data from conversations to build a personalized user profile.\n",
    "\n",
    "[Extraction Criteria] Extract details ONLY if they fall into these categories:\n",
    "- Explicit Preferences: Likes, dislikes, dietary restrictions, favorite items/media.\n",
    "- Biographical current or historic facts: Name, location, job, age, family members, pets, facts about previous life.\n",
    "- Recurring Routines: Daily habits, schedules, frequent activities.\n",
    "- Future Intent: Specific upcoming plans, goals, or milestones.\n",
    "\n",
    "[Exclusion Criteria]\n",
    "- Ignore temporary states (e.g., \"I am hungry now\").\n",
    "- Ignore general conversation topics or opinions unless they indicate a strong preference.\n",
    "- Ignore summaries of the chat.\n",
    "\n",
    "[Format Constraints]\n",
    "- Output strictly a bulleted list.\n",
    "- Do not include introductory or concluding text.\n",
    "- The facts that you store should contain all the relevant context that make that piece of data wholesome.\n",
    "- If no relevant data is found, output \"None\".\n",
    "\n",
    "[Input Conversation]\n",
    "\"\"\"\n",
    "\n",
    "samples_convs_with_memories = []\n",
    "for i, conv in enumerate(tqdm(samples_convs, desc=\"Processing conversations\")):\n",
    "    prompt = base_prompt + \"\\n\".join([flatten_turn(turn) for turn in conv['turns']])\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    conv['memories'] = response.choices[0].message.content\n",
    "    samples_convs_with_memories.append(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0f2138c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation history successfully saved to **summarised_conversation_debug_memories.json**.\n"
     ]
    }
   ],
   "source": [
    "save_conversation_history(samples_convs_with_memories, 'summarised_conversation_debug_memories.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed9b8680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "class Memory():\n",
    "    \"\"\"Stores conversation history with full and summarized versions.\"\"\"\n",
    "\n",
    "    def __init__(self, memory: dict, conversations: list):\n",
    "        self.creation = datetime.now().isoformat()\n",
    "        self.memory = memory\n",
    "        self.full_conversation, self.summarised_conversation = self.parse_conversations(conversations)\n",
    "\n",
    "    def parse_conversations(self, conversations: list) -> tuple[list, list]:\n",
    "        \"\"\"Split conversations into full and summarized versions.\"\"\"\n",
    "        full_convs = []\n",
    "        summarised_convs = []\n",
    "\n",
    "        for conv in conversations:\n",
    "            # Full conversation: all messages with complete content\n",
    "            full_turns = []\n",
    "            summarised_turns = []\n",
    "\n",
    "            for turn in conv.get('turns', []):\n",
    "                if 'user' in turn:\n",
    "                    # User messages: same in both versions\n",
    "                    full_turns.append({'role': 'user', 'content': turn['user']})\n",
    "                    summarised_turns.append({'role': 'user', 'content': turn['user']})\n",
    "\n",
    "                elif 'assistant' in turn:\n",
    "                    # Assistant messages: full vs summarized\n",
    "                    full_turns.append({'role': 'assistant', 'content': turn['assistant']})\n",
    "\n",
    "                    if 'summarised_conversation' in turn:\n",
    "                        summarised_turns.append({\n",
    "                            'role': 'assistant',\n",
    "                            'content': turn['summarised_conversation']\n",
    "                        })\n",
    "\n",
    "            # Store conversations with metadata\n",
    "            full_convs.append({\n",
    "                'id': conv['id'],\n",
    "                'title': conv['title'],\n",
    "                'create_time': conv.get('create_time'),\n",
    "                'update_time': conv.get('update_time'),\n",
    "                'turns': full_turns\n",
    "            })\n",
    "\n",
    "            summarised_convs.append({\n",
    "                'id': conv['id'],\n",
    "                'title': conv['title'],\n",
    "                'create_time': conv.get('create_time'),\n",
    "                'update_time': conv.get('update_time'),\n",
    "                'turns': summarised_turns\n",
    "            })\n",
    "\n",
    "        return full_convs, summarised_convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad63de80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
